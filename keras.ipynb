{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/georgeliu1998/keras_model_tuning/blob/master/keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "QQl1lUm7zmN9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import pickle\n",
        "from time import time\n",
        "import importlib # for dynamic class instantiation from a string\n",
        "\n",
        "#from sklearn import datasets\n",
        "#from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "#from sklearn.metrics import confusion_matrix, mean_squared_error\n",
        "#from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import xgboost as xgb\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "#from keras import optimizers\n",
        "#from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pYJCnFnjwiLF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "seed = np.random.RandomState(6)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wwRIfdxNJ6oM",
        "colab_type": "code",
        "outputId": "c395d0f1-3845-47f5-a222-1eaf28b80834",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/pima-indians-diabetes.csv\", header=None)\n",
        "df.head()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   0    1   2   3    4     5      6   7  8\n",
              "0  6  148  72  35    0  33.6  0.627  50  1\n",
              "1  1   85  66  29    0  26.6  0.351  31  0\n",
              "2  8  183  64   0    0  23.3  0.672  32  1\n",
              "3  1   89  66  23   94  28.1  0.167  21  0\n",
              "4  0  137  40  35  168  43.1  2.288  33  1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "eJ-kaPbGKG7B",
        "colab_type": "code",
        "outputId": "ee63a61d-32c7-4929-bd3e-e57382dc671d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "X, y = df.values[:, 0:8], df.values[:, 8]\n",
        "print(X.shape, y.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(768, 8) (768,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Che2aHnO5Q6v",
        "colab_type": "code",
        "outputId": "9ce1d874-27bd-41f7-b02a-b47d75d3a2d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf = xgb.XGBClassifier()\n",
        "cv = StratifiedKFold(n_splits=3, random_state=seed)\n",
        "\n",
        "scores = cross_val_score(clf, X, y, cv=cv)\n",
        "\n",
        "print(\"Mean Accuracy: {:.2%}, Standard Deviation: {:.2%}\".format(scores.mean(), scores.std()))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean Accuracy: 77.61%, Standard Deviation: 3.07%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HCQ1-GuByxYx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SequentialModel:\n",
        "  \n",
        "  def __init__(self, input_dim, num_layers, num_units, \n",
        "               activation, activation_out, \n",
        "               loss, initializer, optimizer, learning_rate, \n",
        "               metrics, epochs, batch_size, one_hot=False):\n",
        "    \"\"\"\n",
        "    Params:\n",
        "      input_dim: int, number of features\n",
        "      num_layers: int, number of layers of the model (excluding the input layer)\n",
        "      num_units: list, number of units in each layer(excluding the input layer)\n",
        "      activation: str, activation function used in all layers except output\n",
        "      activation_out: str, activation function used in output layer\n",
        "      loss: str, loss functon\n",
        "      initializer: str, kernel initializer\n",
        "      optimizer: str, optimizer\n",
        "      metrics: list of strings, metrics used\n",
        "      epochs: int, number of epochs to train for\n",
        "      batch_size: int, number of samples per batch\n",
        "      one_hot: bool, whether one hot encoding is needed\n",
        "    \"\"\"\n",
        "    self.input_dim = input_dim\n",
        "    self.num_layers = num_layers\n",
        "    self.num_units = num_units\n",
        "    self.activation = activation\n",
        "    self.activation_out = activation_out\n",
        "    self.loss = loss\n",
        "    self.initializer = initializer\n",
        "    self.optimizer = optimizer\n",
        "    self.learning_rate = learning_rate\n",
        "    self.metrics = metrics\n",
        "    self.epochs = epochs\n",
        "    self.batch_size = batch_size\n",
        "    self.one_hot = one_hot\n",
        "    \n",
        "    # Initialize the sequential model\n",
        "    self.model = Sequential()\n",
        "  \n",
        "    \n",
        "  def build_model(self):\n",
        "    \"\"\"\n",
        "    Adds layers and compiles the model\n",
        "    \"\"\"\n",
        "    # Ensure num_units tuple's length is the same as num_layers\n",
        "    if self.num_layers != len(self.num_units):\n",
        "      # Expand the list by repeating number of nodes except for last layer\n",
        "      num_nodes, num_nodes_out = self.num_units[0], self.num_units[-1]\n",
        "      self.num_units = [i for i in range(num_layers-1) for i in [num_nodes]]\n",
        "      self.num_units.append(num_nodes_out) \n",
        "    \n",
        "    # Loop thru all the layers\n",
        "    for i in range(self.num_layers):\n",
        "      # Different layers should have different setups\n",
        "      if i == 0: # first layer\n",
        "        self.model.add(Dense(units=self.num_units[i],\n",
        "                             input_dim=self.input_dim,\n",
        "                             kernel_initializer=initializer,\n",
        "                             activation=activation)) \n",
        "      elif i+1 == self.num_layers: # output layer\n",
        "        self.model.add(Dense(units=self.num_units[i],\n",
        "                             kernel_initializer=initializer,\n",
        "                             activation=activation_out))\n",
        "      else:\n",
        "        self.model.add(Dense(units=self.num_units[i],\n",
        "                            kernel_initializer=initializer,\n",
        "                            activation=activation))\n",
        "    \n",
        "    # Instantiate the optimizer class\n",
        "    optimizer_class = getattr(importlib.import_module(\"keras.optimizers\"), \n",
        "                             self.optimizer)\n",
        "    self.optimizer = optimizer_class(lr=self.learning_rate)\n",
        "    # Compile the model\n",
        "    self.model.compile(loss=self.loss,\n",
        "                       optimizer=self.optimizer,\n",
        "                       metrics=self.metrics)\n",
        "      \n",
        "  \n",
        "  \n",
        "  def evaluate_model(self, X, y, n_splits=3):\n",
        "    \"\"\"\n",
        "    Evaluates the model using cross-validation.\n",
        "    \n",
        "    Params:\n",
        "      X: np.array, features\n",
        "      y: np.array, labels\n",
        "      n_splits: int, number of folds for the cross-validation\n",
        "    Returns:\n",
        "      mean_accuracy: float, the average accuracy based on the cross-validation.\n",
        "    \n",
        "    \"\"\"\n",
        "    score_lst = []\n",
        "    t1 = time()\n",
        "    \n",
        "    print(\"Starting {}-fold cross-validation...\".format(n_splits))\n",
        "    \n",
        "    kfold = StratifiedKFold(n_splits=n_splits, \n",
        "                            shuffle=True, \n",
        "                            random_state=seed)\n",
        "    \n",
        "    # Loop through the different folds\n",
        "    for train_index, test_index in kfold.split(X, y):\n",
        "      # Do one-hot encoding when needed\n",
        "      if self.one_hot:\n",
        "        y_one_hot = to_categorical(y)\n",
        "      else:\n",
        "        y_one_hot = y\n",
        "        \n",
        "      self.model.fit(X[train_index],\n",
        "                     y_one_hot[train_index],\n",
        "                     epochs=self.epochs,\n",
        "                     batch_size=self.batch_size,\n",
        "                     verbose=0)\n",
        "        \n",
        "      scores = self.model.evaluate(X[test_index],\n",
        "                                   y_one_hot[test_index], \n",
        "                                   verbose=0)\n",
        "            \n",
        "      # The second item is accuracy\n",
        "      score_lst.append(scores[1])\n",
        "\n",
        "    t2 = time()\n",
        "    t = t2 - t1\n",
        "    # Convert time to mintues\n",
        "    t /= 60\n",
        "\n",
        "    print(\"Finished cross-valiation. Took {:.1f} mintues.\".format(t))\n",
        "\n",
        "    # Convert to np.array and calculate mean and sd\n",
        "    score_lst = np.array(score_lst)\n",
        "    mean_acc = score_lst.mean()\n",
        "    sd_acc = score_lst.std()\n",
        "\n",
        "    print(\"Mean Accuracy: {:.2%}, Standard Deviation: {:.2%}\".format(mean_acc, sd_acc))\n",
        "    return mean_acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LzC2z1JMyuv5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "!pip install kaggle\n",
        "api_token = {\"username\":\"georgeliu\",\"key\":\"API_KEY\"}\n",
        "import json\n",
        "import zipfile\n",
        "import os\n",
        "with open('/content/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(api_token, file)\n",
        "!chmod 600 /content/.kaggle/kaggle.json\n",
        "!kaggle config path -p /content\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3QbFJkQVK4tQ",
        "colab_type": "code",
        "outputId": "1b3bef8e-22be-45af-c39e-9d2a6de3ee80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "input_dim = 8\n",
        "num_layers = 2\n",
        "num_units = (8, 1) \n",
        "activation = 'relu'\n",
        "activation_out = 'sigmoid'\n",
        "loss = 'binary_crossentropy'\n",
        "initializer = 'random_uniform'\n",
        "optimizer = 'adam'\n",
        "learning_rate = 0.001\n",
        "metrics = ['accuracy']\n",
        "epochs = 10\n",
        "batch_size = 6\n",
        "one_hot = False\n",
        "\n",
        "\n",
        "model = SequentialModel(input_dim=input_dim, \n",
        "                        num_layers=num_layers, \n",
        "                        num_units=num_units,\n",
        "                        activation=activation, \n",
        "                        activation_out=activation_out, \n",
        "                        loss=loss, \n",
        "                        initializer=initializer, \n",
        "                        optimizer=optimizer, \n",
        "                        learning_rate=learning_rate, \n",
        "                        metrics=metrics, \n",
        "                        epochs=epochs, \n",
        "                        batch_size=batch_size, \n",
        "                        one_hot=one_hot)\n",
        "\n",
        "model.build_model()\n",
        "model.evaluate_model(X, y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting 3-fold cross-validation...\n",
            "Finished cross-valiation. Took 0.3 mintues.\n",
            "Mean Accuracy: 69.67%, Standard Deviation: 1.72%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6966674129438014"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "metadata": {
        "id": "2VnF5lzmLISj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "width = [4, 8, 16, 32, 64]\n",
        "depth = [2, 4, 8, 16, 32]\n",
        "loss = ['binary_crossentropy', 'categorical_crossentropy', 'sparse_categorical_crossentropy']\n",
        "initializer = ['random_uniform', 'random_normal', 'TruncatedNormal', 'glorot_normal', 'glorot_uniform']\n",
        "learning_rate = [0.001, 0.002, 0.01, 0.1, 1]\n",
        "optimizer = ['adam', 'adamax', 'adagrad', 'sgd', 'rmsprop']\n",
        "epochs = [10, 20, 40, 80, 160]\n",
        "batch_size = [1, 2, 4, 8, 16]\n",
        "\n",
        "\n",
        "tuning_options = {'width': width,\n",
        "                  'depth': depth, \n",
        "                  'loss': loss, \n",
        "                  'initializer': initializer, \n",
        "                  'optimizer': optimizer, \n",
        "                  'learning_rate': learning_rate,\n",
        "                  'epochs': epochs, \n",
        "                  'batch_size': batch_size}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cf3kbnDie18w",
        "colab_type": "code",
        "outputId": "c74dbce1-7b46-4dc7-dc4f-5d479e5962aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "cell_type": "code",
      "source": [
        "results = {}\n",
        "\n",
        "for parameter, options in tuning_options.items():\n",
        "  \n",
        "  results[parameter] = {}\n",
        "  \n",
        "  input_dim = 8\n",
        "  num_layers = 2\n",
        "  num_units = (8, 1) \n",
        "  activation = 'relu'\n",
        "  activation_out = 'sigmoid'\n",
        "  loss = 'binary_crossentropy'\n",
        "  initializer = 'random_uniform'\n",
        "  learning_rate = 0.001\n",
        "  optimizer = 'adam'\n",
        "  metrics = ['accuracy']\n",
        "  epochs = 10\n",
        "  batch_size = 5\n",
        "  one_hot = False\n",
        "\n",
        "  for option in options:\n",
        "    \n",
        "    if parameter == 'width':\n",
        "      num_units = (option, 1)\n",
        "    elif parameter == 'depth':\n",
        "      num_layers = option\n",
        "    elif parameter == 'loss':\n",
        "      loss = option\n",
        "    elif parameter == 'initializer':\n",
        "      initializer = option\n",
        "    elif parameter == 'optimizer':\n",
        "      optimizer = option\n",
        "    elif parameter == 'learning_rate':\n",
        "      learning_rate = option\n",
        "    elif parameter == 'epochs':\n",
        "      epochs = option\n",
        "    else:\n",
        "      batch_size = option\n",
        "    \n",
        "    print(\"\\nEvaluating parameter \\\"{}\\\" using value \\\"{}\\\"...\".format(parameter, option))\n",
        "    \n",
        "    model = SequentialModel(input_dim=input_dim, \n",
        "                            num_layers=num_layers, \n",
        "                            num_units=num_units,\n",
        "                            activation=activation, \n",
        "                            activation_out=activation_out, \n",
        "                            loss=loss, \n",
        "                            initializer=initializer, \n",
        "                            optimizer=optimizer, \n",
        "                            learning_rate=learning_rate, \n",
        "                            metrics=metrics, \n",
        "                            epochs=epochs, \n",
        "                            batch_size=batch_size, \n",
        "                            one_hot=one_hot)\n",
        "    \n",
        "    try:\n",
        "      model.build_model()\n",
        "      result = model.evaluate_model(X_pima, y_pima)  \n",
        "      results[parameter][option] = result\n",
        "    except:\n",
        "      results[parameter][option] = 'NaN'\n",
        "      print('Error, skipped.')\n",
        "      pass\n",
        "\n",
        "# Save the dict    \n",
        "with open('cross_validation_results.pkl', 'wb') as f:\n",
        "    pickle.dump(results, f, pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating parameter \"width\" using value \"4\"...\n",
            "Starting 3-fold cross-validation...\n",
            "Finished cross-valiation. Took 0.3 mintues.\n",
            "Mean Accuracy: 69.80%, Standard Deviation: 3.31%\n",
            "\n",
            "Evaluating parameter \"width\" using value \"8\"...\n",
            "Starting 3-fold cross-validation...\n",
            "Finished cross-valiation. Took 0.3 mintues.\n",
            "Mean Accuracy: 67.97%, Standard Deviation: 0.10%\n",
            "\n",
            "Evaluating parameter \"width\" using value \"16\"...\n",
            "Starting 3-fold cross-validation...\n",
            "Error, skipped.\n",
            "\n",
            "Evaluating parameter \"width\" using value \"32\"...\n",
            "Starting 3-fold cross-validation...\n",
            "Error, skipped.\n",
            "\n",
            "Evaluating parameter \"width\" using value \"64\"...\n",
            "Starting 3-fold cross-validation...\n",
            "Finished cross-valiation. Took 0.3 mintues.\n",
            "Mean Accuracy: 69.02%, Standard Deviation: 3.03%\n",
            "\n",
            "Evaluating parameter \"depth\" using value \"2\"...\n",
            "Starting 3-fold cross-validation...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Kozw8MO7Iu3I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open('cross_validation_results.pkl', 'rb') as f:\n",
        "    tuning_results = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PTns86tjLRDq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def wrangle(tuning_results):\n",
        "    \"\"\"\n",
        "    Params:\n",
        "        tuning_results: dict, the dict loaded from pickled hyperparameter tuning results\n",
        "    \n",
        "    Returns:\n",
        "        df_long: pandas df, the wrangled long format dataframe\n",
        "    \"\"\"\n",
        "    # Save as df\n",
        "    df = pd.DataFrame(tuning_results)\n",
        "    # Get the col names as value vars for melt func\n",
        "    value_vars = df.columns.tolist()\n",
        "    # Reset index and rename the index col \n",
        "    df = df.reset_index().rename(columns={'index': 'option'})\n",
        "    # Transform from wide to long format for easy plotting\n",
        "    df_long = pd.melt(df, id_vars='option', value_vars=value_vars)\n",
        "    df_long = df_long.rename(columns={'variable': 'parameter'})\n",
        "    # Exclude the zero and null values\n",
        "    df_long = df_long[~df_long['value'].isnull()]\n",
        "    df_long = df_long.query(\"value!=0 & value!='NaN'\")\n",
        "    \n",
        "    return df_long"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jvE7lBvXg1w-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_long = wrangle(tuning_results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q_xgwDiwjvHr",
        "colab_type": "code",
        "outputId": "e2c68bef-610b-41a8-ff20-536d80727068",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        }
      },
      "cell_type": "code",
      "source": [
        "# Calculate the range and sd of each parameter group and convert to a df\n",
        "ranges = df_long.groupby('parameter').apply(lambda grp: grp.value.max() - grp.value.min())\n",
        "sd = df_long.groupby('parameter').apply(lambda grp: grp.value.std())\n",
        "spread = pd.concat([ranges, sd], axis=1).rename(columns={0: 'ranges', 1: 'sd'})\n",
        "spread"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ranges</th>\n",
              "      <th>sd</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>parameter</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>batch_size</th>\n",
              "      <td>0.041688</td>\n",
              "      <td>0.016619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>depth</th>\n",
              "      <td>0.055995</td>\n",
              "      <td>0.026254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>epochs</th>\n",
              "      <td>0.042883</td>\n",
              "      <td>0.018743</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>initializer</th>\n",
              "      <td>0.066351</td>\n",
              "      <td>0.028171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>learning_rate</th>\n",
              "      <td>0.345127</td>\n",
              "      <td>0.143832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>loss</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>optimizer</th>\n",
              "      <td>0.043040</td>\n",
              "      <td>0.015726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>width</th>\n",
              "      <td>0.076839</td>\n",
              "      <td>0.027471</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 ranges        sd\n",
              "parameter                        \n",
              "batch_size     0.041688  0.016619\n",
              "depth          0.055995  0.026254\n",
              "epochs         0.042883  0.018743\n",
              "initializer    0.066351  0.028171\n",
              "learning_rate  0.345127  0.143832\n",
              "loss           0.000000       NaN\n",
              "optimizer      0.043040  0.015726\n",
              "width          0.076839  0.027471"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "M4jzd1Aaj9uE",
        "colab_type": "code",
        "outputId": "6988dc63-7a45-46c4-f97c-6e88f2cadf7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1165
        }
      },
      "cell_type": "code",
      "source": [
        "# Join back the df\n",
        "df_spread = pd.merge(df_long, spread, how='left', left_on='parameter', right_index=True)\n",
        "df_spread"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>option</th>\n",
              "      <th>parameter</th>\n",
              "      <th>value</th>\n",
              "      <th>ranges</th>\n",
              "      <th>sd</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>batch_size</td>\n",
              "      <td>0.651043</td>\n",
              "      <td>0.041688</td>\n",
              "      <td>0.016619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>batch_size</td>\n",
              "      <td>0.671943</td>\n",
              "      <td>0.041688</td>\n",
              "      <td>0.016619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>batch_size</td>\n",
              "      <td>0.68887</td>\n",
              "      <td>0.041688</td>\n",
              "      <td>0.016619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8</td>\n",
              "      <td>batch_size</td>\n",
              "      <td>0.692731</td>\n",
              "      <td>0.041688</td>\n",
              "      <td>0.016619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>16</td>\n",
              "      <td>batch_size</td>\n",
              "      <td>0.681144</td>\n",
              "      <td>0.041688</td>\n",
              "      <td>0.016619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>2</td>\n",
              "      <td>depth</td>\n",
              "      <td>0.707038</td>\n",
              "      <td>0.055995</td>\n",
              "      <td>0.026254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>4</td>\n",
              "      <td>depth</td>\n",
              "      <td>0.687558</td>\n",
              "      <td>0.055995</td>\n",
              "      <td>0.026254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>8</td>\n",
              "      <td>depth</td>\n",
              "      <td>0.651043</td>\n",
              "      <td>0.055995</td>\n",
              "      <td>0.026254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>16</td>\n",
              "      <td>depth</td>\n",
              "      <td>0.651043</td>\n",
              "      <td>0.055995</td>\n",
              "      <td>0.026254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>32</td>\n",
              "      <td>depth</td>\n",
              "      <td>0.651043</td>\n",
              "      <td>0.055995</td>\n",
              "      <td>0.026254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>10</td>\n",
              "      <td>epochs</td>\n",
              "      <td>0.703214</td>\n",
              "      <td>0.042883</td>\n",
              "      <td>0.018743</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>20</td>\n",
              "      <td>epochs</td>\n",
              "      <td>0.709653</td>\n",
              "      <td>0.042883</td>\n",
              "      <td>0.018743</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>40</td>\n",
              "      <td>epochs</td>\n",
              "      <td>0.717542</td>\n",
              "      <td>0.042883</td>\n",
              "      <td>0.018743</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>80</td>\n",
              "      <td>epochs</td>\n",
              "      <td>0.746096</td>\n",
              "      <td>0.042883</td>\n",
              "      <td>0.018743</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>160</td>\n",
              "      <td>epochs</td>\n",
              "      <td>0.739509</td>\n",
              "      <td>0.042883</td>\n",
              "      <td>0.018743</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>random_uniform</td>\n",
              "      <td>initializer</td>\n",
              "      <td>0.688962</td>\n",
              "      <td>0.066351</td>\n",
              "      <td>0.028171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>random_normal</td>\n",
              "      <td>initializer</td>\n",
              "      <td>0.694058</td>\n",
              "      <td>0.066351</td>\n",
              "      <td>0.028171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>TruncatedNormal</td>\n",
              "      <td>initializer</td>\n",
              "      <td>0.709648</td>\n",
              "      <td>0.066351</td>\n",
              "      <td>0.028171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>glorot_normal</td>\n",
              "      <td>initializer</td>\n",
              "      <td>0.643297</td>\n",
              "      <td>0.066351</td>\n",
              "      <td>0.028171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>glorot_uniform</td>\n",
              "      <td>initializer</td>\n",
              "      <td>0.653698</td>\n",
              "      <td>0.066351</td>\n",
              "      <td>0.028171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>1</td>\n",
              "      <td>learning_rate</td>\n",
              "      <td>0.348957</td>\n",
              "      <td>0.345127</td>\n",
              "      <td>0.143832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>0.001</td>\n",
              "      <td>learning_rate</td>\n",
              "      <td>0.694083</td>\n",
              "      <td>0.345127</td>\n",
              "      <td>0.143832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133</th>\n",
              "      <td>0.002</td>\n",
              "      <td>learning_rate</td>\n",
              "      <td>0.675854</td>\n",
              "      <td>0.345127</td>\n",
              "      <td>0.143832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>0.01</td>\n",
              "      <td>learning_rate</td>\n",
              "      <td>0.651043</td>\n",
              "      <td>0.345127</td>\n",
              "      <td>0.143832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135</th>\n",
              "      <td>0.1</td>\n",
              "      <td>learning_rate</td>\n",
              "      <td>0.651043</td>\n",
              "      <td>0.345127</td>\n",
              "      <td>0.143832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165</th>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>loss</td>\n",
              "      <td>0.662823</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>adam</td>\n",
              "      <td>optimizer</td>\n",
              "      <td>0.694078</td>\n",
              "      <td>0.043040</td>\n",
              "      <td>0.015726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>adamax</td>\n",
              "      <td>optimizer</td>\n",
              "      <td>0.681103</td>\n",
              "      <td>0.043040</td>\n",
              "      <td>0.015726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>adagrad</td>\n",
              "      <td>optimizer</td>\n",
              "      <td>0.651038</td>\n",
              "      <td>0.043040</td>\n",
              "      <td>0.015726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200</th>\n",
              "      <td>sgd</td>\n",
              "      <td>optimizer</td>\n",
              "      <td>0.677116</td>\n",
              "      <td>0.043040</td>\n",
              "      <td>0.015726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>201</th>\n",
              "      <td>rmsprop</td>\n",
              "      <td>optimizer</td>\n",
              "      <td>0.67972</td>\n",
              "      <td>0.043040</td>\n",
              "      <td>0.015726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>205</th>\n",
              "      <td>4</td>\n",
              "      <td>width</td>\n",
              "      <td>0.651043</td>\n",
              "      <td>0.076839</td>\n",
              "      <td>0.027471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>206</th>\n",
              "      <td>8</td>\n",
              "      <td>width</td>\n",
              "      <td>0.684913</td>\n",
              "      <td>0.076839</td>\n",
              "      <td>0.027471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>208</th>\n",
              "      <td>16</td>\n",
              "      <td>width</td>\n",
              "      <td>0.686149</td>\n",
              "      <td>0.076839</td>\n",
              "      <td>0.027471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>210</th>\n",
              "      <td>32</td>\n",
              "      <td>width</td>\n",
              "      <td>0.727882</td>\n",
              "      <td>0.076839</td>\n",
              "      <td>0.027471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>231</th>\n",
              "      <td>64</td>\n",
              "      <td>width</td>\n",
              "      <td>0.695422</td>\n",
              "      <td>0.076839</td>\n",
              "      <td>0.027471</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  option      parameter     value    ranges        sd\n",
              "0                      1     batch_size  0.651043  0.041688  0.016619\n",
              "1                      2     batch_size  0.671943  0.041688  0.016619\n",
              "2                      4     batch_size   0.68887  0.041688  0.016619\n",
              "3                      8     batch_size  0.692731  0.041688  0.016619\n",
              "5                     16     batch_size  0.681144  0.041688  0.016619\n",
              "30                     2          depth  0.707038  0.055995  0.026254\n",
              "31                     4          depth  0.687558  0.055995  0.026254\n",
              "32                     8          depth  0.651043  0.055995  0.026254\n",
              "34                    16          depth  0.651043  0.055995  0.026254\n",
              "36                    32          depth  0.651043  0.055995  0.026254\n",
              "62                    10         epochs  0.703214  0.042883  0.018743\n",
              "64                    20         epochs  0.709653  0.042883  0.018743\n",
              "66                    40         epochs  0.717542  0.042883  0.018743\n",
              "67                    80         epochs  0.746096  0.042883  0.018743\n",
              "68                   160         epochs  0.739509  0.042883  0.018743\n",
              "98        random_uniform    initializer  0.688962  0.066351  0.028171\n",
              "99         random_normal    initializer  0.694058  0.066351  0.028171\n",
              "100      TruncatedNormal    initializer  0.709648  0.066351  0.028171\n",
              "101        glorot_normal    initializer  0.643297  0.066351  0.028171\n",
              "102       glorot_uniform    initializer  0.653698  0.066351  0.028171\n",
              "116                    1  learning_rate  0.348957  0.345127  0.143832\n",
              "132                0.001  learning_rate  0.694083  0.345127  0.143832\n",
              "133                0.002  learning_rate  0.675854  0.345127  0.143832\n",
              "134                 0.01  learning_rate  0.651043  0.345127  0.143832\n",
              "135                  0.1  learning_rate  0.651043  0.345127  0.143832\n",
              "165  binary_crossentropy           loss  0.662823  0.000000       NaN\n",
              "197                 adam      optimizer  0.694078  0.043040  0.015726\n",
              "198               adamax      optimizer  0.681103  0.043040  0.015726\n",
              "199              adagrad      optimizer  0.651038  0.043040  0.015726\n",
              "200                  sgd      optimizer  0.677116  0.043040  0.015726\n",
              "201              rmsprop      optimizer   0.67972  0.043040  0.015726\n",
              "205                    4          width  0.651043  0.076839  0.027471\n",
              "206                    8          width  0.684913  0.076839  0.027471\n",
              "208                   16          width  0.686149  0.076839  0.027471\n",
              "210                   32          width  0.727882  0.076839  0.027471\n",
              "231                   64          width  0.695422  0.076839  0.027471"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "56iZjE-Y1BBr",
        "colab_type": "code",
        "outputId": "d8bc948c-a191-4e01-ae87-2f2d9f883c16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1165
        }
      },
      "cell_type": "code",
      "source": [
        "# Reorder columns\n",
        "df_spread = df_spread[['parameter', 'option', 'value', 'ranges', 'sd']]\n",
        "df_spread"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>parameter</th>\n",
              "      <th>option</th>\n",
              "      <th>value</th>\n",
              "      <th>ranges</th>\n",
              "      <th>sd</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>batch_size</td>\n",
              "      <td>1</td>\n",
              "      <td>0.651043</td>\n",
              "      <td>0.041688</td>\n",
              "      <td>0.016619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>batch_size</td>\n",
              "      <td>2</td>\n",
              "      <td>0.671943</td>\n",
              "      <td>0.041688</td>\n",
              "      <td>0.016619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>batch_size</td>\n",
              "      <td>4</td>\n",
              "      <td>0.68887</td>\n",
              "      <td>0.041688</td>\n",
              "      <td>0.016619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>batch_size</td>\n",
              "      <td>8</td>\n",
              "      <td>0.692731</td>\n",
              "      <td>0.041688</td>\n",
              "      <td>0.016619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>batch_size</td>\n",
              "      <td>16</td>\n",
              "      <td>0.681144</td>\n",
              "      <td>0.041688</td>\n",
              "      <td>0.016619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>depth</td>\n",
              "      <td>2</td>\n",
              "      <td>0.707038</td>\n",
              "      <td>0.055995</td>\n",
              "      <td>0.026254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>depth</td>\n",
              "      <td>4</td>\n",
              "      <td>0.687558</td>\n",
              "      <td>0.055995</td>\n",
              "      <td>0.026254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>depth</td>\n",
              "      <td>8</td>\n",
              "      <td>0.651043</td>\n",
              "      <td>0.055995</td>\n",
              "      <td>0.026254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>depth</td>\n",
              "      <td>16</td>\n",
              "      <td>0.651043</td>\n",
              "      <td>0.055995</td>\n",
              "      <td>0.026254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>depth</td>\n",
              "      <td>32</td>\n",
              "      <td>0.651043</td>\n",
              "      <td>0.055995</td>\n",
              "      <td>0.026254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>epochs</td>\n",
              "      <td>10</td>\n",
              "      <td>0.703214</td>\n",
              "      <td>0.042883</td>\n",
              "      <td>0.018743</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>epochs</td>\n",
              "      <td>20</td>\n",
              "      <td>0.709653</td>\n",
              "      <td>0.042883</td>\n",
              "      <td>0.018743</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>epochs</td>\n",
              "      <td>40</td>\n",
              "      <td>0.717542</td>\n",
              "      <td>0.042883</td>\n",
              "      <td>0.018743</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>epochs</td>\n",
              "      <td>80</td>\n",
              "      <td>0.746096</td>\n",
              "      <td>0.042883</td>\n",
              "      <td>0.018743</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>epochs</td>\n",
              "      <td>160</td>\n",
              "      <td>0.739509</td>\n",
              "      <td>0.042883</td>\n",
              "      <td>0.018743</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>initializer</td>\n",
              "      <td>random_uniform</td>\n",
              "      <td>0.688962</td>\n",
              "      <td>0.066351</td>\n",
              "      <td>0.028171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>initializer</td>\n",
              "      <td>random_normal</td>\n",
              "      <td>0.694058</td>\n",
              "      <td>0.066351</td>\n",
              "      <td>0.028171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>initializer</td>\n",
              "      <td>TruncatedNormal</td>\n",
              "      <td>0.709648</td>\n",
              "      <td>0.066351</td>\n",
              "      <td>0.028171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>initializer</td>\n",
              "      <td>glorot_normal</td>\n",
              "      <td>0.643297</td>\n",
              "      <td>0.066351</td>\n",
              "      <td>0.028171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>initializer</td>\n",
              "      <td>glorot_uniform</td>\n",
              "      <td>0.653698</td>\n",
              "      <td>0.066351</td>\n",
              "      <td>0.028171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>learning_rate</td>\n",
              "      <td>1</td>\n",
              "      <td>0.348957</td>\n",
              "      <td>0.345127</td>\n",
              "      <td>0.143832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>learning_rate</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.694083</td>\n",
              "      <td>0.345127</td>\n",
              "      <td>0.143832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133</th>\n",
              "      <td>learning_rate</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.675854</td>\n",
              "      <td>0.345127</td>\n",
              "      <td>0.143832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>learning_rate</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.651043</td>\n",
              "      <td>0.345127</td>\n",
              "      <td>0.143832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135</th>\n",
              "      <td>learning_rate</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.651043</td>\n",
              "      <td>0.345127</td>\n",
              "      <td>0.143832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165</th>\n",
              "      <td>loss</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>0.662823</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>optimizer</td>\n",
              "      <td>adam</td>\n",
              "      <td>0.694078</td>\n",
              "      <td>0.043040</td>\n",
              "      <td>0.015726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>optimizer</td>\n",
              "      <td>adamax</td>\n",
              "      <td>0.681103</td>\n",
              "      <td>0.043040</td>\n",
              "      <td>0.015726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>optimizer</td>\n",
              "      <td>adagrad</td>\n",
              "      <td>0.651038</td>\n",
              "      <td>0.043040</td>\n",
              "      <td>0.015726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200</th>\n",
              "      <td>optimizer</td>\n",
              "      <td>sgd</td>\n",
              "      <td>0.677116</td>\n",
              "      <td>0.043040</td>\n",
              "      <td>0.015726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>201</th>\n",
              "      <td>optimizer</td>\n",
              "      <td>rmsprop</td>\n",
              "      <td>0.67972</td>\n",
              "      <td>0.043040</td>\n",
              "      <td>0.015726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>205</th>\n",
              "      <td>width</td>\n",
              "      <td>4</td>\n",
              "      <td>0.651043</td>\n",
              "      <td>0.076839</td>\n",
              "      <td>0.027471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>206</th>\n",
              "      <td>width</td>\n",
              "      <td>8</td>\n",
              "      <td>0.684913</td>\n",
              "      <td>0.076839</td>\n",
              "      <td>0.027471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>208</th>\n",
              "      <td>width</td>\n",
              "      <td>16</td>\n",
              "      <td>0.686149</td>\n",
              "      <td>0.076839</td>\n",
              "      <td>0.027471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>210</th>\n",
              "      <td>width</td>\n",
              "      <td>32</td>\n",
              "      <td>0.727882</td>\n",
              "      <td>0.076839</td>\n",
              "      <td>0.027471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>231</th>\n",
              "      <td>width</td>\n",
              "      <td>64</td>\n",
              "      <td>0.695422</td>\n",
              "      <td>0.076839</td>\n",
              "      <td>0.027471</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         parameter               option     value    ranges        sd\n",
              "0       batch_size                    1  0.651043  0.041688  0.016619\n",
              "1       batch_size                    2  0.671943  0.041688  0.016619\n",
              "2       batch_size                    4   0.68887  0.041688  0.016619\n",
              "3       batch_size                    8  0.692731  0.041688  0.016619\n",
              "5       batch_size                   16  0.681144  0.041688  0.016619\n",
              "30           depth                    2  0.707038  0.055995  0.026254\n",
              "31           depth                    4  0.687558  0.055995  0.026254\n",
              "32           depth                    8  0.651043  0.055995  0.026254\n",
              "34           depth                   16  0.651043  0.055995  0.026254\n",
              "36           depth                   32  0.651043  0.055995  0.026254\n",
              "62          epochs                   10  0.703214  0.042883  0.018743\n",
              "64          epochs                   20  0.709653  0.042883  0.018743\n",
              "66          epochs                   40  0.717542  0.042883  0.018743\n",
              "67          epochs                   80  0.746096  0.042883  0.018743\n",
              "68          epochs                  160  0.739509  0.042883  0.018743\n",
              "98     initializer       random_uniform  0.688962  0.066351  0.028171\n",
              "99     initializer        random_normal  0.694058  0.066351  0.028171\n",
              "100    initializer      TruncatedNormal  0.709648  0.066351  0.028171\n",
              "101    initializer        glorot_normal  0.643297  0.066351  0.028171\n",
              "102    initializer       glorot_uniform  0.653698  0.066351  0.028171\n",
              "116  learning_rate                    1  0.348957  0.345127  0.143832\n",
              "132  learning_rate                0.001  0.694083  0.345127  0.143832\n",
              "133  learning_rate                0.002  0.675854  0.345127  0.143832\n",
              "134  learning_rate                 0.01  0.651043  0.345127  0.143832\n",
              "135  learning_rate                  0.1  0.651043  0.345127  0.143832\n",
              "165           loss  binary_crossentropy  0.662823  0.000000       NaN\n",
              "197      optimizer                 adam  0.694078  0.043040  0.015726\n",
              "198      optimizer               adamax  0.681103  0.043040  0.015726\n",
              "199      optimizer              adagrad  0.651038  0.043040  0.015726\n",
              "200      optimizer                  sgd  0.677116  0.043040  0.015726\n",
              "201      optimizer              rmsprop   0.67972  0.043040  0.015726\n",
              "205          width                    4  0.651043  0.076839  0.027471\n",
              "206          width                    8  0.684913  0.076839  0.027471\n",
              "208          width                   16  0.686149  0.076839  0.027471\n",
              "210          width                   32  0.727882  0.076839  0.027471\n",
              "231          width                   64  0.695422  0.076839  0.027471"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "id": "sCeY1-cn3dCZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Remove the row with zero value\n",
        "df_spread = df_spread.query('ranges!=0')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cTjLQCNWxReH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Reset index\n",
        "#df_spread.reset_index(drop=True, inplace=True)\n",
        "idx = df_spread.sort_values(by=['ranges', 'option'], ascending=False).index\n",
        "df_plot = df_spread.loc[idx, :]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bFR1PQL4zVJH",
        "colab_type": "code",
        "outputId": "8d2b224b-a7f3-4cb8-d5e1-63fe5dfadd57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1134
        }
      },
      "cell_type": "code",
      "source": [
        "df_plot"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>parameter</th>\n",
              "      <th>option</th>\n",
              "      <th>value</th>\n",
              "      <th>ranges</th>\n",
              "      <th>sd</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>learning_rate</td>\n",
              "      <td>1</td>\n",
              "      <td>0.348957</td>\n",
              "      <td>0.345127</td>\n",
              "      <td>0.143832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135</th>\n",
              "      <td>learning_rate</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.651043</td>\n",
              "      <td>0.345127</td>\n",
              "      <td>0.143832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>learning_rate</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.651043</td>\n",
              "      <td>0.345127</td>\n",
              "      <td>0.143832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133</th>\n",
              "      <td>learning_rate</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.675854</td>\n",
              "      <td>0.345127</td>\n",
              "      <td>0.143832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>learning_rate</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.694083</td>\n",
              "      <td>0.345127</td>\n",
              "      <td>0.143832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>231</th>\n",
              "      <td>width</td>\n",
              "      <td>64</td>\n",
              "      <td>0.695422</td>\n",
              "      <td>0.076839</td>\n",
              "      <td>0.027471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>210</th>\n",
              "      <td>width</td>\n",
              "      <td>32</td>\n",
              "      <td>0.727882</td>\n",
              "      <td>0.076839</td>\n",
              "      <td>0.027471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>208</th>\n",
              "      <td>width</td>\n",
              "      <td>16</td>\n",
              "      <td>0.686149</td>\n",
              "      <td>0.076839</td>\n",
              "      <td>0.027471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>206</th>\n",
              "      <td>width</td>\n",
              "      <td>8</td>\n",
              "      <td>0.684913</td>\n",
              "      <td>0.076839</td>\n",
              "      <td>0.027471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>205</th>\n",
              "      <td>width</td>\n",
              "      <td>4</td>\n",
              "      <td>0.651043</td>\n",
              "      <td>0.076839</td>\n",
              "      <td>0.027471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>initializer</td>\n",
              "      <td>random_uniform</td>\n",
              "      <td>0.688962</td>\n",
              "      <td>0.066351</td>\n",
              "      <td>0.028171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>initializer</td>\n",
              "      <td>random_normal</td>\n",
              "      <td>0.694058</td>\n",
              "      <td>0.066351</td>\n",
              "      <td>0.028171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>initializer</td>\n",
              "      <td>glorot_uniform</td>\n",
              "      <td>0.653698</td>\n",
              "      <td>0.066351</td>\n",
              "      <td>0.028171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>initializer</td>\n",
              "      <td>glorot_normal</td>\n",
              "      <td>0.643297</td>\n",
              "      <td>0.066351</td>\n",
              "      <td>0.028171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>initializer</td>\n",
              "      <td>TruncatedNormal</td>\n",
              "      <td>0.709648</td>\n",
              "      <td>0.066351</td>\n",
              "      <td>0.028171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>depth</td>\n",
              "      <td>32</td>\n",
              "      <td>0.651043</td>\n",
              "      <td>0.055995</td>\n",
              "      <td>0.026254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>depth</td>\n",
              "      <td>16</td>\n",
              "      <td>0.651043</td>\n",
              "      <td>0.055995</td>\n",
              "      <td>0.026254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>depth</td>\n",
              "      <td>8</td>\n",
              "      <td>0.651043</td>\n",
              "      <td>0.055995</td>\n",
              "      <td>0.026254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>depth</td>\n",
              "      <td>4</td>\n",
              "      <td>0.687558</td>\n",
              "      <td>0.055995</td>\n",
              "      <td>0.026254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>depth</td>\n",
              "      <td>2</td>\n",
              "      <td>0.707038</td>\n",
              "      <td>0.055995</td>\n",
              "      <td>0.026254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200</th>\n",
              "      <td>optimizer</td>\n",
              "      <td>sgd</td>\n",
              "      <td>0.677116</td>\n",
              "      <td>0.043040</td>\n",
              "      <td>0.015726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>201</th>\n",
              "      <td>optimizer</td>\n",
              "      <td>rmsprop</td>\n",
              "      <td>0.67972</td>\n",
              "      <td>0.043040</td>\n",
              "      <td>0.015726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>optimizer</td>\n",
              "      <td>adamax</td>\n",
              "      <td>0.681103</td>\n",
              "      <td>0.043040</td>\n",
              "      <td>0.015726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>optimizer</td>\n",
              "      <td>adam</td>\n",
              "      <td>0.694078</td>\n",
              "      <td>0.043040</td>\n",
              "      <td>0.015726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>optimizer</td>\n",
              "      <td>adagrad</td>\n",
              "      <td>0.651038</td>\n",
              "      <td>0.043040</td>\n",
              "      <td>0.015726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>epochs</td>\n",
              "      <td>160</td>\n",
              "      <td>0.739509</td>\n",
              "      <td>0.042883</td>\n",
              "      <td>0.018743</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>epochs</td>\n",
              "      <td>80</td>\n",
              "      <td>0.746096</td>\n",
              "      <td>0.042883</td>\n",
              "      <td>0.018743</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>epochs</td>\n",
              "      <td>40</td>\n",
              "      <td>0.717542</td>\n",
              "      <td>0.042883</td>\n",
              "      <td>0.018743</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>epochs</td>\n",
              "      <td>20</td>\n",
              "      <td>0.709653</td>\n",
              "      <td>0.042883</td>\n",
              "      <td>0.018743</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>epochs</td>\n",
              "      <td>10</td>\n",
              "      <td>0.703214</td>\n",
              "      <td>0.042883</td>\n",
              "      <td>0.018743</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>batch_size</td>\n",
              "      <td>16</td>\n",
              "      <td>0.681144</td>\n",
              "      <td>0.041688</td>\n",
              "      <td>0.016619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>batch_size</td>\n",
              "      <td>8</td>\n",
              "      <td>0.692731</td>\n",
              "      <td>0.041688</td>\n",
              "      <td>0.016619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>batch_size</td>\n",
              "      <td>4</td>\n",
              "      <td>0.68887</td>\n",
              "      <td>0.041688</td>\n",
              "      <td>0.016619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>batch_size</td>\n",
              "      <td>2</td>\n",
              "      <td>0.671943</td>\n",
              "      <td>0.041688</td>\n",
              "      <td>0.016619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>batch_size</td>\n",
              "      <td>1</td>\n",
              "      <td>0.651043</td>\n",
              "      <td>0.041688</td>\n",
              "      <td>0.016619</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         parameter           option     value    ranges        sd\n",
              "116  learning_rate                1  0.348957  0.345127  0.143832\n",
              "135  learning_rate              0.1  0.651043  0.345127  0.143832\n",
              "134  learning_rate             0.01  0.651043  0.345127  0.143832\n",
              "133  learning_rate            0.002  0.675854  0.345127  0.143832\n",
              "132  learning_rate            0.001  0.694083  0.345127  0.143832\n",
              "231          width               64  0.695422  0.076839  0.027471\n",
              "210          width               32  0.727882  0.076839  0.027471\n",
              "208          width               16  0.686149  0.076839  0.027471\n",
              "206          width                8  0.684913  0.076839  0.027471\n",
              "205          width                4  0.651043  0.076839  0.027471\n",
              "98     initializer   random_uniform  0.688962  0.066351  0.028171\n",
              "99     initializer    random_normal  0.694058  0.066351  0.028171\n",
              "102    initializer   glorot_uniform  0.653698  0.066351  0.028171\n",
              "101    initializer    glorot_normal  0.643297  0.066351  0.028171\n",
              "100    initializer  TruncatedNormal  0.709648  0.066351  0.028171\n",
              "36           depth               32  0.651043  0.055995  0.026254\n",
              "34           depth               16  0.651043  0.055995  0.026254\n",
              "32           depth                8  0.651043  0.055995  0.026254\n",
              "31           depth                4  0.687558  0.055995  0.026254\n",
              "30           depth                2  0.707038  0.055995  0.026254\n",
              "200      optimizer              sgd  0.677116  0.043040  0.015726\n",
              "201      optimizer          rmsprop   0.67972  0.043040  0.015726\n",
              "198      optimizer           adamax  0.681103  0.043040  0.015726\n",
              "197      optimizer             adam  0.694078  0.043040  0.015726\n",
              "199      optimizer          adagrad  0.651038  0.043040  0.015726\n",
              "68          epochs              160  0.739509  0.042883  0.018743\n",
              "67          epochs               80  0.746096  0.042883  0.018743\n",
              "66          epochs               40  0.717542  0.042883  0.018743\n",
              "64          epochs               20  0.709653  0.042883  0.018743\n",
              "62          epochs               10  0.703214  0.042883  0.018743\n",
              "5       batch_size               16  0.681144  0.041688  0.016619\n",
              "3       batch_size                8  0.692731  0.041688  0.016619\n",
              "2       batch_size                4   0.68887  0.041688  0.016619\n",
              "1       batch_size                2  0.671943  0.041688  0.016619\n",
              "0       batch_size                1  0.651043  0.041688  0.016619"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "e5SS5w_7DXZs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "outputId": "ef59cca5-20ec-4101-f0dd-847915038756"
      },
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(16, 12))\n",
        "df_plot['value'] = df_plot['value'].astype(float)\n",
        "sns.boxplot(x='parameter', y='value', data=df_plot, ax=ax)\n",
        "sns.swarmplot(x='parameter', y='value', data=df_plot, size=6, ax=ax)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/seaborn/categorical.py:454: FutureWarning: remove_na is deprecated and is a private function. Do not use.\n",
            "  box_data = remove_na(group_data)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fa442c2b240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7MAAAK5CAYAAABt8nm9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngJkXAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fa442c3d7f0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "xPHlALpxJdrh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}